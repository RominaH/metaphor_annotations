{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3af32a7-e5ae-4103-adeb-6c3ae59c2dbe",
   "metadata": {},
   "source": [
    "# Tokenizing Comments and POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54b998-82d6-4fd6-b382-22a24a82c767",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8463c3-a059-4fb0-824c-8876fdb0539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ff8d0c-3d0f-4339-ad08-ed75c34d1a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# spacy pipeline \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8418cfd-ea27-4863-a1b7-0cda2d6c6906",
   "metadata": {},
   "source": [
    "## Reading and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a269280a-94c0-401a-9b15-28f34ee9a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in EDITED metaphor annotation json and save it to a pandas dataframe\n",
    "file_path = 'project-2-at-2025-05-20-edit.json'\n",
    "met_df = pd.read_json(file_path)\n",
    "\n",
    "# cleaning file names\n",
    "met_df['filename'] = met_df.apply(lambda row: re.sub(r\"^[^_]*-\", '', row.file_upload), axis=1)\n",
    "met_df['filename'] = met_df.apply(lambda row: re.sub(r\"_fixed\", '', row.filename[:-4]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7f5a4-6444-44e9-bcc0-e224f2e87acb",
   "metadata": {},
   "source": [
    "## Tokenizing and POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85aeb056-5f6c-4f87-bb6c-8172e12c03b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romha\\AppData\\Local\\Temp\\ipykernel_3480\\3084724230.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_analysis['text'] = df_analysis.apply(lambda row: row.data['text'], axis=1)\n",
      "C:\\Users\\romha\\AppData\\Local\\Temp\\ipykernel_3480\\3084724230.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_analysis['sentences'] = df_analysis.apply(lambda row: [sent.text.strip() for sent in nlp(row.text).sents], axis=1)\n",
      "C:\\Users\\romha\\AppData\\Local\\Temp\\ipykernel_3480\\3084724230.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_analysis['tokens'] = df_analysis.apply(lambda row: tokenizer(row.text), axis=1)\n",
      "C:\\Users\\romha\\AppData\\Local\\Temp\\ipykernel_3480\\3084724230.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_analysis['pos'] = df_analysis.apply(lambda row: [(token.text, token.pos_) for token in nlp(row.text)], axis=1)\n",
      "C:\\Users\\romha\\AppData\\Local\\Temp\\ipykernel_3480\\3084724230.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_analysis['pos_sentence'] = df_analysis.apply(lambda row: [[(token.text, token.pos_) for token in nlp(sent)] for sent in row.sentences], axis=1)\n",
      "C:\\Users\\romha\\AppData\\Local\\Temp\\ipykernel_3480\\3084724230.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_analysis['metaphor_tokens']=df_analysis.apply(lambda row: [[str(token) for token in tokenizer(metaphor['value']['text'])] for metaphor in row.annotations[0]['result']], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# creating a new dataframe based on the metaphor dataframe, with fewer columns\n",
    "df_analysis=met_df[['filename', 'data', 'annotations']]\n",
    "# extracting comment text from data column \n",
    "df_analysis['text'] = df_analysis.apply(lambda row: row.data['text'], axis=1)\n",
    "# creating a column of text split into sentences\n",
    "df_analysis['sentences'] = df_analysis.apply(lambda row: [sent.text.strip() for sent in nlp(row.text).sents], axis=1)\n",
    "# creating a column of text split into tokens\n",
    "df_analysis['tokens'] = df_analysis.apply(lambda row: tokenizer(row.text), axis=1)\n",
    "# creating a column with POS tagging\n",
    "df_analysis['pos'] = df_analysis.apply(lambda row: [(token.text, token.pos_) for token in nlp(row.text)], axis=1)\n",
    "# creating a column with POS tagging per sentence\n",
    "df_analysis['pos_sentence'] = df_analysis.apply(lambda row: [[(token.text, token.pos_) for token in nlp(sent)] for sent in row.sentences], axis=1)\n",
    "# creating a column with metaphor tokens\n",
    "df_analysis['metaphor_tokens']=df_analysis.apply(lambda row: [[str(token) for token in tokenizer(metaphor['value']['text'])] for metaphor in row.annotations[0]['result']], axis=1)\n",
    "# dropping data column\n",
    "df_analysis = df_analysis.drop(columns=['data', 'annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03d318c-0acf-49ae-b1d6-578eb8ae3bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos</th>\n",
       "      <th>pos_sentence</th>\n",
       "      <th>metaphor_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hillary_1</td>\n",
       "      <td>' The problem is bigger than Mr. Trump.'Extend...</td>\n",
       "      <td>[' The problem is bigger than Mr. Trump., 'Ext...</td>\n",
       "      <td>(', The, problem, is, bigger, than, Mr., Trump...</td>\n",
       "      <td>[(', PUNCT), (The, DET), (problem, NOUN), (is,...</td>\n",
       "      <td>[[(', PUNCT), (The, DET), (problem, NOUN), (is...</td>\n",
       "      <td>[[blue, collar]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hillary_2</td>\n",
       "      <td>'If she was a man, she would be president-elec...</td>\n",
       "      <td>['If she was a man, she would be president-ele...</td>\n",
       "      <td>('If, she, was, a, man,, she, would, be, presi...</td>\n",
       "      <td>[(', PUNCT), (If, SCONJ), (she, PRON), (was, A...</td>\n",
       "      <td>[[(', PUNCT), (If, SCONJ), (she, PRON), (was, ...</td>\n",
       "      <td>[[genitals]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hillary_3</td>\n",
       "      <td>'What are you going to tell your daughters?'Te...</td>\n",
       "      <td>['What are you going to tell your daughters?'T...</td>\n",
       "      <td>('What, are, you, going, to, tell, your, daugh...</td>\n",
       "      <td>[(', PUNCT), (What, PRON), (are, AUX), (you, P...</td>\n",
       "      <td>[[(', PUNCT), (What, PRON), (are, AUX), (you, ...</td>\n",
       "      <td>[[engage]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hillary_4</td>\n",
       "      <td>'What are you going to tell your daughters?'We...</td>\n",
       "      <td>['What are you going to tell your daughters?'W...</td>\n",
       "      <td>('What, are, you, going, to, tell, your, daugh...</td>\n",
       "      <td>[(', PUNCT), (What, PRON), (are, AUX), (you, P...</td>\n",
       "      <td>[[(', PUNCT), (What, PRON), (are, AUX), (you, ...</td>\n",
       "      <td>[[top], [third, world]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hillary_5</td>\n",
       "      <td>A PR piece designed to reverse all the damage ...</td>\n",
       "      <td>[A PR piece designed to reverse all the damage...</td>\n",
       "      <td>(A, PR, piece, designed, to, reverse, all, the...</td>\n",
       "      <td>[(A, DET), (PR, NOUN), (piece, NOUN), (designe...</td>\n",
       "      <td>[[(A, DET), (PR, NOUN), (piece, NOUN), (design...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filename                                               text  \\\n",
       "0  hillary_1  ' The problem is bigger than Mr. Trump.'Extend...   \n",
       "1  hillary_2  'If she was a man, she would be president-elec...   \n",
       "2  hillary_3  'What are you going to tell your daughters?'Te...   \n",
       "3  hillary_4  'What are you going to tell your daughters?'We...   \n",
       "4  hillary_5  A PR piece designed to reverse all the damage ...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [' The problem is bigger than Mr. Trump., 'Ext...   \n",
       "1  ['If she was a man, she would be president-ele...   \n",
       "2  ['What are you going to tell your daughters?'T...   \n",
       "3  ['What are you going to tell your daughters?'W...   \n",
       "4  [A PR piece designed to reverse all the damage...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  (', The, problem, is, bigger, than, Mr., Trump...   \n",
       "1  ('If, she, was, a, man,, she, would, be, presi...   \n",
       "2  ('What, are, you, going, to, tell, your, daugh...   \n",
       "3  ('What, are, you, going, to, tell, your, daugh...   \n",
       "4  (A, PR, piece, designed, to, reverse, all, the...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [(', PUNCT), (The, DET), (problem, NOUN), (is,...   \n",
       "1  [(', PUNCT), (If, SCONJ), (she, PRON), (was, A...   \n",
       "2  [(', PUNCT), (What, PRON), (are, AUX), (you, P...   \n",
       "3  [(', PUNCT), (What, PRON), (are, AUX), (you, P...   \n",
       "4  [(A, DET), (PR, NOUN), (piece, NOUN), (designe...   \n",
       "\n",
       "                                        pos_sentence          metaphor_tokens  \n",
       "0  [[(', PUNCT), (The, DET), (problem, NOUN), (is...         [[blue, collar]]  \n",
       "1  [[(', PUNCT), (If, SCONJ), (she, PRON), (was, ...             [[genitals]]  \n",
       "2  [[(', PUNCT), (What, PRON), (are, AUX), (you, ...               [[engage]]  \n",
       "3  [[(', PUNCT), (What, PRON), (are, AUX), (you, ...  [[top], [third, world]]  \n",
       "4  [[(A, DET), (PR, NOUN), (piece, NOUN), (design...                       []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00aa346e-406d-4c4c-9f61-ba658f47b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.to_csv('tokenized_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e2b7a-68ef-489b-ad92-c4b21f03b9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
