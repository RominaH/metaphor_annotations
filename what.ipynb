{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POcTijMFrlD2"
   },
   "source": [
    "# Script for comparing annotation\n",
    "Currently, we are looking to compare the annotations between annotators regarding metaphors and non-metaphors. Note that all words/phrases that are tagged are only tagged with the label [METAPHOR]. We want a script that:\n",
    "1. Compares words/phases that get tagged\n",
    "2. Compares the tags associated with these words/phrases\n",
    "3. Outputs some sort of agreement level between annotators\n",
    "\n",
    "In the future, we also want some way to compare these annotations with the appraisal annotations for these articles to see what class of words tend to be classified as metaphors or non-metaphors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HehCnqwNvDD0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statistics import mean\n",
    "from ast import literal_eval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo3IsnQMsQ9n"
   },
   "source": [
    "## Load annotations from LabelStudio output\n",
    "LabelStudio can output the annotations in various formats, including json, csv, and tsv.\n",
    "\n",
    "For simplicity we are using the csv option, which includes the following columns:\n",
    "* **annotation_id** - unique id associated with each annotation\n",
    "* **annotator** - id associated with a particular annotator\n",
    "* **created_at** - time the annotation was created\n",
    "* **id** - text id\n",
    "* **label** - dictionary of labels associated with the text\n",
    "  * \"start\": the index of the first labelled character within the selected text\n",
    "  * \"end\": the index of the final labelled character within the selected text\n",
    "  * \"text\": the text (word/phrase) that was selected to be labelled\n",
    "  * \"labels\": a list of labels associated with the selected text\n",
    "* **lead_time** - (not sure what this is)\n",
    "* **text** - article text\n",
    "* **updated_at** - time the annotation was last updated\n",
    "\n",
    "Relevant columns here are annotator, id (rename to text_id), label, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\romha\\\\OneDrive - University of Waterloo\\\\Desktop\\\\Spr25\\\\metaphor_annotations'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Yw4fSAB_u7sg"
   },
   "outputs": [],
   "source": [
    "input = 'mar4.csv'\n",
    "annotations_df = pd.read_csv(input, usecols=[\"annotator\", \"id\", \"label\", \"text\", \"created_at\", \"updated_at\"])\n",
    "# rename 'id' to 'text_id' for clarification\n",
    "annotations_df.rename(columns={\"id\": \"text_id\"}, inplace=True)\n",
    "\n",
    "# reorder columns for clarification\n",
    "order = [\"text_id\", \"annotator\", \"text\", \"label\"]\n",
    "annotations_df = annotations_df.reindex(columns=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-9dkFOJzO6I"
   },
   "source": [
    "## Comparing tagged words and labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeNaZDYcAvQi"
   },
   "source": [
    "Tentative idea for how to proceed:\n",
    "* With just two annotators, we could manually split the annotations into two dataframes, and then merge them again on text id, giving us a new dataframe containing the text id, the article text, and two label columns from the annotators\n",
    "* From there, we can write a function to compare the lists of dictionaries in the label columns, and then create the following additional columns in this new merged dataframe:\n",
    "  * overlap_agreement - if there is an overlap between non-identical tags (e.g. \"line\" vs \"line in the sand\"), how much do they agree?\n",
    "  * textual_agreement - of words/phrases tagged, how many are identical?\n",
    "\n",
    "To filter by annotators, use the numbers found from the previous code block and replace them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7KW0szom5mKk"
   },
   "outputs": [],
   "source": [
    "# filter the dataframe to contain just copies of annotations from each annotator\n",
    "annotator1_df = annotations_df.loc[(annotations_df['annotator'] == 4)]\n",
    "annotator2_df = annotations_df.loc[(annotations_df['annotator'] == 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-kdNtqGVZCy"
   },
   "source": [
    "Here we create a few helper functions to turn the label columns into useable data.\n",
    "\n",
    "First, `labels_to_list` takes in the labels columns and returns a list of lists containing just the start and end indices of tagged words/phrases. These uniquely identify a tagged item within the text, and disambiguates between different instances of the same word (e.g. if \"line\" appears multiple times throughout the comment), so it is all the information we will need to work with.\n",
    "\n",
    "Each item in the returned list is a list of [start index, end index pairs], extracted from the list of dictionaries of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Z1s0ugHq4IBp"
   },
   "outputs": [],
   "source": [
    "# helper function to build a list of lists containing the start and end indices\n",
    "# and the difference between these indices\n",
    "# where labels is the labels associated with a specific text\n",
    "def labels_to_list(labels):\n",
    "  annotations = []\n",
    "\n",
    "  labels = literal_eval(labels)\n",
    "\n",
    "  for label in labels:\n",
    "    tags = []\n",
    "    tags.append(int(label['start']))\n",
    "    tags.append(int(label['end']))\n",
    "\n",
    "    annotations.append(tags)\n",
    "\n",
    "  return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwgczKeuV-SS"
   },
   "source": [
    "This is the main helper function that will produce textual and overlap agreement. As a reminder, textual agreement refers to the similarity in items that are tagged between the annotators (we are not counting the number of tags, but rather, the specific words that are tagged - it would not be very useful to know that they share the same number of annotations if they are both tagging different words). Overlap agreement looks specifically at items that may overlap, for example, if one annotator only tagged \"line\" as metaphor but the other tagged the entire phrase \"line in the sand\".\n",
    "\n",
    "To determine the textual agreement, we first make the labels dictionaries into usable start-end index pairs, making sure to sort them in order (the csv output seems to list the labels in the order that they are created, not in the order the text appears in the comment). Then, we iterate through the two lists to create two additional lists - these contain the annotations that will later be used to compare against each other to produce cohen's kappa.\n",
    "\n",
    "To produce the additional lists, we add in order the labels that appear in both lists; when a label only appears in one list, we append \"0, 0\" to the other list to denote that the item was not annotated by that annotator.\n",
    "\n",
    "It is trickier to use cohen's kappa to determine overlap agreement. As such, I used a rudimentary/brute force method where for each overlaping (but, crucially, not identical) label, I find the ratio between the difference of the indices and add this to a third list for easier calculation.\n",
    "\n",
    "As an example: let's say \"line\" has indices [0, 3] and \"line in the sand\" has indices [0, 15]. Taking the difference, \"line\" has a difference of `3` and \"line in the sand\" has a differnece of `15`. We then divide `3/15` to get `0.2`, telling us that the two tags only overlap by `20%`.\n",
    "\n",
    "Once these ratios are calculated, we then find the mean of these overlap ratios to determine how closely the overlapping tags were to each other, with 0 being no overlap at all and 1 being 100% overlap (note: a 1 should never appear, as for comments where there is no overlap between annotations, `NaN` is instead appended to the row to avoid messing up final calculations).\n",
    "\n",
    "Similarly, a 1 in textual agreement suggests 100% agreement, and a 0 indicates 0% agreement between the annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eeKdIk7ybmWF"
   },
   "outputs": [],
   "source": [
    "# helper function to compare tags and labels for a single text\n",
    "# where row contains the text and labels from both annotators associated with the text\n",
    "# label1 and label2 are the column headers for the labels\n",
    "# returns a list containing the agreement score\n",
    "# of overlapping labels and overall tags\n",
    "def compare_labels(row, label1, label2):\n",
    "\n",
    "  # turn labels dictionaries into usable lists\n",
    "  list1 = labels_to_list(row[label1])\n",
    "  list2 = labels_to_list(row[label2])\n",
    "\n",
    "  # sort the keys to be in index order\n",
    "  # since LabelStudio orders it by time updated, not order of appearance in text\n",
    "  list1 = sorted(list1, key=lambda x: x[0])\n",
    "  list2 = sorted(list2, key=lambda x: x[0])\n",
    "\n",
    "  tags1 = []\n",
    "  tags2 = []\n",
    "  overlap = []\n",
    "\n",
    "  i = 0\n",
    "  j = 0\n",
    "\n",
    "  # cohen's kappa takes two lists of equal lengths to compare labels\n",
    "  # so here we are building new lists from the annotations\n",
    "  # appending 0's where the label exists in one list but not the other\n",
    "  # and iterating through both lists until we have read both of them in full\n",
    "\n",
    "  while i < len(list1) and j < len(list2):\n",
    "    # if the current label in list1 appears before the current label in list2\n",
    "    # append list1's label to tags1\n",
    "    # and append the '0, 0' label to tags2\n",
    "    if list1[i][0] < list2[j][0]:\n",
    "      tags1.append(str(list1[i]))\n",
    "      tags2.append(\"0, 0\")\n",
    "      i += 1\n",
    "\n",
    "    # if the current label in list 1 starts with the same word instance\n",
    "    # as the label in list2\n",
    "    # append both labels to their respective lists\n",
    "    elif list1[i][0] == list2[j][0]:\n",
    "      tags1.append(str(list1[i]))\n",
    "      tags2.append(str(list2[j]))\n",
    "\n",
    "      # and here we check for overlap with the difference in index\n",
    "      # e.g. if one label is [4, 10] and the other [4, 16]\n",
    "      # we compare the difference of 6 and 12\n",
    "      # and append 6/12, or 0.5, to the overlap list\n",
    "      if list1[i][1] != list2[j][1]:\n",
    "        diff1 = list1[i][1] - list1[i][0]\n",
    "        diff2 = list2[j][1] - list2[j][0]\n",
    "        numerator = min(diff1, diff2)\n",
    "        denominator = max(diff1, diff2)\n",
    "        overlap.append(float(numerator/denominator))\n",
    "\n",
    "      i += 1\n",
    "      j += 1\n",
    "\n",
    "    # if the current label in list2 appears before the current label in list1\n",
    "    # append list2's label to tags2\n",
    "    # and append the '0, 0' label to tags1\n",
    "    else:\n",
    "      tags1.append(\"0, 0\")\n",
    "      tags2.append(str(list2[j]))\n",
    "      j += 1\n",
    "\n",
    "  # find cohen's kappa using our two tags arrays\n",
    "  # if the tags arrays only contain 1 element (i.e. only has one label)\n",
    "  # cohen's kappa returns a NaN\n",
    "  # so the condition checks and corrects for that\n",
    "  textual_kappa = cohen_kappa_score(tags1, tags2)\n",
    "  if tags1 == tags2 and len(tags1) == 1:\n",
    "    textual_kappa = 1\n",
    "\n",
    "  # find the mean of the overlap agreement in the text\n",
    "  # or append NaN if no overlap\n",
    "  if overlap != []:\n",
    "    overlap_agreement = mean(overlap)\n",
    "  else:\n",
    "    overlap_agreement = np.nan\n",
    "\n",
    "  return [textual_kappa, overlap_agreement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>textual_agreement</th>\n",
       "      <th>overlap_agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>Nobody really cares if a woman or man holds th...</td>\n",
       "      <td>[{\"start\":322,\"end\":326,\"text\":\"bill\",\"labels\"...</td>\n",
       "      <td>[{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>Funnily enough, if the Ontario Liberals tried ...</td>\n",
       "      <td>[{\"start\":48,\"end\":58,\"text\":\"shell game\",\"lab...</td>\n",
       "      <td>[{\"start\":48,\"end\":58,\"text\":\"shell game\",\"lab...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>Apple stuff consist of 95% marketing nonsense ...</td>\n",
       "      <td>[{\"start\":138,\"end\":144,\"text\":\"thrive\",\"label...</td>\n",
       "      <td>[{\"start\":125,\"end\":130,\"text\":\"knows\",\"labels...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>This whole concept of 'national daycare' sound...</td>\n",
       "      <td>[{\"start\":72,\"end\":76,\"text\":\"he##\",\"labels\":[...</td>\n",
       "      <td>[{\"start\":294,\"end\":302,\"text\":\"draining\",\"lab...</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>185</td>\n",
       "      <td>'... a larger question is how to deal with Chi...</td>\n",
       "      <td>[{\"start\":139,\"end\":143,\"text\":\"sold\",\"labels\"...</td>\n",
       "      <td>[{\"start\":64,\"end\":69,\"text\":\"petty\",\"labels\":...</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                               text  \\\n",
       "1       72  Nobody really cares if a woman or man holds th...   \n",
       "2      131  Funnily enough, if the Ontario Liberals tried ...   \n",
       "3      146  Apple stuff consist of 95% marketing nonsense ...   \n",
       "4      178  This whole concept of 'national daycare' sound...   \n",
       "5      185  '... a larger question is how to deal with Chi...   \n",
       "\n",
       "                                             label_1  \\\n",
       "1  [{\"start\":322,\"end\":326,\"text\":\"bill\",\"labels\"...   \n",
       "2  [{\"start\":48,\"end\":58,\"text\":\"shell game\",\"lab...   \n",
       "3  [{\"start\":138,\"end\":144,\"text\":\"thrive\",\"label...   \n",
       "4  [{\"start\":72,\"end\":76,\"text\":\"he##\",\"labels\":[...   \n",
       "5  [{\"start\":139,\"end\":143,\"text\":\"sold\",\"labels\"...   \n",
       "\n",
       "                                             label_2  textual_agreement  \\\n",
       "1  [{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels...           1.000000   \n",
       "2  [{\"start\":48,\"end\":58,\"text\":\"shell game\",\"lab...           1.000000   \n",
       "3  [{\"start\":125,\"end\":130,\"text\":\"knows\",\"labels...           1.000000   \n",
       "4  [{\"start\":294,\"end\":302,\"text\":\"draining\",\"lab...           0.298246   \n",
       "5  [{\"start\":64,\"end\":69,\"text\":\"petty\",\"labels\":...           0.859649   \n",
       "\n",
       "   overlap_agreement  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "5                NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHKd5pV-99uy"
   },
   "source": [
    "Note: dropping NaN rows means that this code only checks for inter-annotator agreement if BOTH annotators have annotated the text. It does not account for cases where one annotator found metaphors in the text and another does not annotate anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6-6gmBFQEdv8"
   },
   "outputs": [],
   "source": [
    "compare_df = pd.merge(annotator1_df, annotator2_df, on=['text_id', 'text'], suffixes=('_1', '_2'))\n",
    "# annotator columns are redundant now\n",
    "compare_df = compare_df.drop(columns=['annotator_1', 'annotator_2'])\n",
    "# drop any rows without annotations\n",
    "compare_df = compare_df.dropna()\n",
    "\n",
    "# compare_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2_QYLdU42HZ",
    "outputId": "7bb48865-9146-48dc-9c97-78901b9da5d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check for how many rows are getting compared\n",
    "len(compare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"start\":322,\"end\":326,\"text\":\"bill\",\"labels\":[\"METAPHOR\"]},{\"start\":314,\"end\":317,\"text\":\"fit\",\"labels\":[\"METAPHOR\"]},{\"start\":38,\"end\":43,\"text\":\"holds\",\"labels\":[\"METAPHOR\"]},{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[322, 326], [314, 317], [38, 43], [48, 55]]\n",
      "[1.0, nan]\n",
      "[{\"start\":48,\"end\":58,\"text\":\"shell game\",\"labels\":[\"METAPHOR\"]},{\"start\":168,\"end\":173,\"text\":\"crack\",\"labels\":[\"METAPHOR\"]},{\"start\":212,\"end\":219,\"text\":\"cracked\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[48, 58], [168, 173], [212, 219]]\n",
      "[1.0, nan]\n",
      "[{\"start\":138,\"end\":144,\"text\":\"thrive\",\"labels\":[\"METAPHOR\"]},{\"start\":53,\"end\":62,\"text\":\"substance\",\"labels\":[\"METAPHOR\"]},{\"start\":125,\"end\":130,\"text\":\"knows\",\"labels\":[\"METAPHOR\"]},{\"start\":155,\"end\":160,\"text\":\"knows\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[138, 144], [53, 62], [125, 130], [155, 160]]\n",
      "[1.0, nan]\n",
      "[{\"start\":72,\"end\":76,\"text\":\"he##\",\"labels\":[\"METAPHOR\"]},{\"start\":294,\"end\":302,\"text\":\"draining\",\"labels\":[\"METAPHOR\"]},{\"start\":670,\"end\":685,\"text\":\"limousine style\",\"labels\":[\"METAPHOR\"]},{\"start\":748,\"end\":753,\"text\":\"tight\",\"labels\":[\"METAPHOR\"]},{\"start\":719,\"end\":726,\"text\":\"liberal\",\"labels\":[\"METAPHOR\"]},{\"start\":614,\"end\":622,\"text\":\"targeted\",\"labels\":[\"METAPHOR\"]},{\"start\":87,\"end\":94,\"text\":\"details\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[72, 76], [294, 302], [670, 685], [748, 753], [719, 726], [614, 622], [87, 94]]\n",
      "[0.29824561403508776, nan]\n",
      "[{\"start\":139,\"end\":143,\"text\":\"sold\",\"labels\":[\"METAPHOR\"]},{\"start\":148,\"end\":152,\"text\":\"soul\",\"labels\":[\"METAPHOR\"]},{\"start\":232,\"end\":239,\"text\":\"rebuild\",\"labels\":[\"METAPHOR\"]},{\"start\":64,\"end\":69,\"text\":\"petty\",\"labels\":[\"METAPHOR\"]},{\"start\":70,\"end\":87,\"text\":\"mean-spiritedness\",\"labels\":[\"METAPHOR\"]},{\"start\":466,\"end\":476,\"text\":\"threatened\",\"labels\":[\"METAPHOR\"]},{\"start\":104,\"end\":111,\"text\":\"Western\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[139, 143], [148, 152], [232, 239], [64, 69], [70, 87], [466, 476], [104, 111]]\n",
      "[0.8596491228070176, nan]\n",
      "[{\"start\":107,\"end\":111,\"text\":\"core\",\"labels\":[\"METAPHOR\"]},{\"start\":237,\"end\":244,\"text\":\"stocked\",\"labels\":[\"METAPHOR\"]},{\"start\":361,\"end\":365,\"text\":\"rock\",\"labels\":[\"METAPHOR\"]},{\"start\":370,\"end\":374,\"text\":\"boat\",\"labels\":[\"METAPHOR\"]},{\"start\":457,\"end\":464,\"text\":\"low key\",\"labels\":[\"METAPHOR\"]},{\"start\":481,\"end\":485,\"text\":\"warm\",\"labels\":[\"METAPHOR\"]},{\"start\":543,\"end\":547,\"text\":\"deal\",\"labels\":[\"METAPHOR\"]},{\"start\":673,\"end\":685,\"text\":\"conservative\",\"labels\":[\"METAPHOR\"]},{\"start\":686,\"end\":690,\"text\":\"fold\",\"labels\":[\"METAPHOR\"]},{\"start\":703,\"end\":707,\"text\":\"open\",\"labels\":[\"METAPHOR\"]},{\"start\":708,\"end\":715,\"text\":\"liberal\",\"labels\":[\"METAPHOR\"]},{\"start\":716,\"end\":721,\"text\":\"place\",\"labels\":[\"METAPHOR\"]},{\"start\":191,\"end\":203,\"text\":\"conservative\",\"labels\":[\"METAPHOR\"]},{\"start\":284,\"end\":296,\"text\":\"conservative\",\"labels\":[\"METAPHOR\"]},{\"start\":141,\"end\":153,\"text\":\"conservative\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[107, 111], [237, 244], [361, 365], [370, 374], [457, 464], [481, 485], [543, 547], [673, 685], [686, 690], [703, 707], [708, 715], [716, 721], [191, 203], [284, 296], [141, 153]]\n",
      "[0.7887323943661971, nan]\n",
      "[{\"start\":70,\"end\":79,\"text\":\"switch up\",\"labels\":[\"METAPHOR\"]},{\"start\":368,\"end\":375,\"text\":\"running\",\"labels\":[\"METAPHOR\"]},{\"start\":376,\"end\":380,\"text\":\"thin\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[70, 79], [368, 375], [376, 380]]\n",
      "[0.07692307692307687, nan]\n",
      "[{\"start\":274,\"end\":278,\"text\":\"cost\",\"labels\":[\"METAPHOR\"]},{\"start\":318,\"end\":325,\"text\":\"look to\",\"labels\":[\"METAPHOR\"]},{\"start\":366,\"end\":370,\"text\":\"join\",\"labels\":[\"METAPHOR\"]},{\"start\":401,\"end\":409,\"text\":\"building\",\"labels\":[\"METAPHOR\"]},{\"start\":603,\"end\":610,\"text\":\"join in\",\"labels\":[\"METAPHOR\"]},{\"start\":660,\"end\":669,\"text\":\"apartheid\",\"labels\":[\"METAPHOR\"]},{\"start\":127,\"end\":133,\"text\":\"broken\",\"labels\":[\"METAPHOR\"]},{\"start\":70,\"end\":81,\"text\":\"main stream\",\"labels\":[\"METAPHOR\"]},{\"start\":168,\"end\":172,\"text\":\"live\",\"labels\":[\"METAPHOR\"]},{\"start\":13,\"end\":18,\"text\":\"clear\",\"labels\":[\"METAPHOR\"]},{\"start\":726,\"end\":733,\"text\":\"grounds\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[274, 278], [318, 325], [366, 370], [401, 409], [603, 610], [660, 669], [127, 133], [70, 81], [168, 172], [13, 18], [726, 733]]\n",
      "[0.3157894736842104, nan]\n",
      "[{\"start\":102,\"end\":111,\"text\":\"defending\",\"labels\":[\"METAPHOR\"]},{\"start\":769,\"end\":773,\"text\":\"cart\",\"labels\":[\"METAPHOR\"]},{\"start\":1369,\"end\":1375,\"text\":\"dumped\",\"labels\":[\"METAPHOR\"]},{\"start\":1258,\"end\":1264,\"text\":\"divest\",\"labels\":[\"METAPHOR\"]},{\"start\":2081,\"end\":2089,\"text\":\"diverted\",\"labels\":[\"METAPHOR\"]},{\"start\":2096,\"end\":2107,\"text\":\"on the take\",\"labels\":[\"METAPHOR\"]},{\"start\":1111,\"end\":1118,\"text\":\"run out\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[102, 111], [769, 773], [1369, 1375], [1258, 1264], [2081, 2089], [2096, 2107], [1111, 1118]]\n",
      "[1.0, nan]\n",
      "[{\"start\":55,\"end\":64,\"text\":\"left wing\",\"labels\":[\"METAPHOR\"]},{\"start\":96,\"end\":106,\"text\":\"right wing\",\"labels\":[\"METAPHOR\"]},{\"start\":43,\"end\":51,\"text\":\"hallmark\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[55, 64], [96, 106], [43, 51]]\n",
      "[1.0, nan]\n",
      "[{\"start\":4,\"end\":10,\"text\":\"buying\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[4, 10]]\n",
      "[1, nan]\n",
      "[{\"start\":33,\"end\":38,\"text\":\"fault\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[33, 38]]\n",
      "[1, nan]\n",
      "[{\"start\":32,\"end\":38,\"text\":\"linked\",\"labels\":[\"METAPHOR\"]},{\"start\":148,\"end\":152,\"text\":\"shoe\",\"labels\":[\"METAPHOR\"]},{\"start\":170,\"end\":174,\"text\":\"foot\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[32, 38], [148, 152], [170, 174]]\n",
      "[1.0, nan]\n",
      "[{\"start\":11,\"end\":22,\"text\":\"imperialism\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[11, 22]]\n",
      "[1, nan]\n",
      "[{\"start\":606,\"end\":612,\"text\":\"backed\",\"labels\":[\"METAPHOR\"]},{\"start\":239,\"end\":247,\"text\":\"accepted\",\"labels\":[\"METAPHOR\"]},{\"start\":419,\"end\":426,\"text\":\"ignored\",\"labels\":[\"METAPHOR\"]},{\"start\":524,\"end\":528,\"text\":\"tell\",\"labels\":[\"METAPHOR\"]},{\"start\":538,\"end\":544,\"text\":\"accept\",\"labels\":[\"METAPHOR\"]},{\"start\":645,\"end\":648,\"text\":\"key\",\"labels\":[\"METAPHOR\"]},{\"start\":7,\"end\":14,\"text\":\"thumbed\",\"labels\":[\"METAPHOR\"]},{\"start\":19,\"end\":23,\"text\":\"nose\",\"labels\":[\"METAPHOR\"]},{\"start\":64,\"end\":73,\"text\":\"judgement\",\"labels\":[\"METAPHOR\"]},{\"start\":127,\"end\":134,\"text\":\"awarded\",\"labels\":[\"METAPHOR\"]},{\"start\":102,\"end\":107,\"text\":\"ruled\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[606, 612], [239, 247], [419, 426], [524, 528], [538, 544], [645, 648], [7, 14], [19, 23], [64, 73], [127, 134], [102, 107]]\n",
      "[1.0, nan]\n",
      "[{\"start\":11,\"end\":16,\"text\":\"limit\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[11, 16]]\n",
      "[1, nan]\n",
      "[{\"start\":61,\"end\":67,\"text\":\"skewed\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[61, 67]]\n",
      "[1, nan]\n",
      "[{\"start\":167,\"end\":173,\"text\":\"failed\",\"labels\":[\"METAPHOR\"]},{\"start\":15,\"end\":22,\"text\":\"failing\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[167, 173], [15, 22]]\n",
      "[1.0, nan]\n",
      "[{\"start\":15,\"end\":20,\"text\":\"close\",\"labels\":[\"METAPHOR\"]},{\"start\":180,\"end\":184,\"text\":\"earn\",\"labels\":[\"METAPHOR\"]},{\"start\":193,\"end\":197,\"text\":\"keep\",\"labels\":[\"METAPHOR\"]},{\"start\":225,\"end\":235,\"text\":\"free lunch\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[15, 20], [180, 184], [193, 197], [225, 235]]\n",
      "[1.0, nan]\n",
      "[{\"start\":277,\"end\":285,\"text\":\"worships\",\"labels\":[\"METAPHOR\"]},{\"start\":450,\"end\":456,\"text\":\"moving\",\"labels\":[\"METAPHOR\"]},{\"start\":176,\"end\":179,\"text\":\"see\",\"labels\":[\"METAPHOR\"]},{\"start\":141,\"end\":149,\"text\":\"The West\",\"labels\":[\"METAPHOR\"]},{\"start\":28,\"end\":39,\"text\":\"Middle East\",\"labels\":[\"METAPHOR\"]},{\"start\":431,\"end\":439,\"text\":\"the West\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[277, 285], [450, 456], [176, 179], [141, 149], [28, 39], [431, 439]]\n",
      "[0.36363636363636354, 0.875]\n",
      "[{\"start\":41,\"end\":49,\"text\":\"balanced\",\"labels\":[\"METAPHOR\"]},{\"start\":66,\"end\":69,\"text\":\"sky\",\"labels\":[\"METAPHOR\"]},{\"start\":73,\"end\":80,\"text\":\"falling\",\"labels\":[\"METAPHOR\"]},{\"start\":122,\"end\":126,\"text\":\"view\",\"labels\":[\"METAPHOR\"]},{\"start\":807,\"end\":815,\"text\":\"turn out\",\"labels\":[\"METAPHOR\"]},{\"start\":1245,\"end\":1249,\"text\":\"kick\",\"labels\":[\"METAPHOR\"]},{\"start\":1256,\"end\":1259,\"text\":\"out\",\"labels\":[\"METAPHOR\"]},{\"start\":1366,\"end\":1373,\"text\":\"rebuild\",\"labels\":[\"METAPHOR\"]},{\"start\":1428,\"end\":1437,\"text\":\"communist\",\"labels\":[\"METAPHOR\"]},{\"start\":1612,\"end\":1622,\"text\":\"free world\",\"labels\":[\"METAPHOR\"]},{\"start\":1638,\"end\":1642,\"text\":\"born\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[41, 49], [66, 69], [73, 80], [122, 126], [807, 815], [1245, 1249], [1256, 1259], [1366, 1373], [1428, 1437], [1612, 1622], [1638, 1642]]\n",
      "[0.4039735099337749, 0.4]\n",
      "[{\"start\":13,\"end\":24,\"text\":\"drama queen\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[13, 24]]\n",
      "[1, nan]\n",
      "[{\"start\":72,\"end\":81,\"text\":\"demanding\",\"labels\":[\"METAPHOR\"]},{\"start\":185,\"end\":189,\"text\":\"fall\",\"labels\":[\"METAPHOR\"]},{\"start\":194,\"end\":200,\"text\":\"demand\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[72, 81], [185, 189], [194, 200]]\n",
      "[0.2857142857142857, nan]\n",
      "[{\"start\":44,\"end\":51,\"text\":\"hurting\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[44, 51]]\n",
      "[1, nan]\n",
      "[{\"start\":419,\"end\":432,\"text\":\"selling point\",\"labels\":[\"METAPHOR\"]}]\n",
      "[[419, 432]]\n",
      "[0.0, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "for index, row in compare_df.iterrows():\n",
    "    r=row['label_1']\n",
    "    print(r)\n",
    "    print(labels_to_list(r))\n",
    "    agreement = compare_labels(row, 'label_1', 'label_2')\n",
    "    print(agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[419, 432]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_list(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tGTZRE5acY_U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "textual_agreement = []\n",
    "overlap_agreement = []\n",
    "\n",
    "for index, row in compare_df.iterrows():\n",
    "  agreement = compare_labels(row, 'label_1', 'label_2')\n",
    "  textual_agreement.append(agreement[0])\n",
    "  overlap_agreement.append(agreement[1])\n",
    "\n",
    "compare_df['textual_agreement'] = textual_agreement\n",
    "compare_df['overlap_agreement'] = overlap_agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRBcHVtjZWXr"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As per [this](https://datatab.net/tutorial/cohens-kappa) explanation of cohen's kappa, below is a guideline for how the result can be interpreted:\n",
    "* \\> 0.8 - almost perfect\n",
    "* \\> 0.6 - substantial\n",
    "* \\> 0.4 - moderate\n",
    "* \\> 0.2 - fair\n",
    "* 0 - 0.2 - slight\n",
    "* < 0 - poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRXKEtYnTwii",
    "outputId": "22810a1a-bcf3-4539-cb93-830c85860059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average textual agreement:  0.7757065536440007\n",
      "Average overlap agreement:  0.6375\n"
     ]
    }
   ],
   "source": [
    "print(\"Average textual agreement: \", compare_df['textual_agreement'].mean())\n",
    "print(\"Average overlap agreement: \", compare_df['overlap_agreement'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[322, 326], [314, 317], [38, 43], [48, 55]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_list('[{\"start\":322,\"end\":326,\"text\":\"bill\",\"labels\":[\"METAPHOR\"]},{\"start\":314,\"end\":317,\"text\":\"fit\",\"labels\":[\"METAPHOR\"]},{\"start\":38,\"end\":43,\"text\":\"holds\",\"labels\":[\"METAPHOR\"]},{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels\":[\"METAPHOR\"]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
