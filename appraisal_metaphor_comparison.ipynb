{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3af32a7-e5ae-4103-adeb-6c3ae59c2dbe",
   "metadata": {},
   "source": [
    "# Appraisal vs Metaphor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54b998-82d6-4fd6-b382-22a24a82c767",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8463c3-a059-4fb0-824c-8876fdb0539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8418cfd-ea27-4863-a1b7-0cda2d6c6906",
   "metadata": {},
   "source": [
    "## Reading and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3255e23a-1998-4c32-b11d-da0760f2c5fc",
   "metadata": {},
   "source": [
    "### Metaphor Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6c8805-bed7-4d1a-9a28-df15a9e1a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed from Jodie\n",
    "# helper function to build a list of lists containing the start and end indices\n",
    "# and the difference between these indices\n",
    "# where labels is the labels associated with a specific text\n",
    "def labels_to_list(labels):\n",
    "  annotations = []\n",
    "\n",
    "  labels = literal_eval(labels)\n",
    "\n",
    "  for label in labels:\n",
    "    tags = []\n",
    "    tags.append(int(label['start']))\n",
    "    tags.append(int(label['end']))\n",
    "\n",
    "    annotations.append(tags)\n",
    "\n",
    "  return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d6eb62-e848-49d4-b539-5ce5dc937ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in metaphor annotation json and save it to a pandas dataframe\n",
    "file_path = 'metaphor_labels_mar26_2025.json'\n",
    "met_df = pd.read_json(file_path)\n",
    "\n",
    "# extracting labels from the annotations column\n",
    "met_df['labels'] = met_df.apply(lambda row: row.annotations[0]['result'], axis=1)\n",
    "\n",
    "# changing file names so they match the appraisal folder names\n",
    "met_df['filename'] = met_df.apply(lambda row: re.sub(r\"^[^_]*-\", '', row.file_upload), axis=1)\n",
    "met_df['filename'] = met_df.apply(lambda row: re.sub(r\"_fixed\", '', row.filename[:-4]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04290f9-009a-42cb-9836-317ca6e881fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of metaphor labels, where each key is a filename\n",
    "met_labels = {}\n",
    "for name in met_df['filename'].unique():\n",
    "    # creating a new dataframe only containing labels corresponding to one file\n",
    "    new_df = met_df[met_df['filename']==name][['filename', 'labels']].reset_index()\n",
    "    # creating a list to save the labels in \n",
    "    labels_list = []\n",
    "    for el in new_df['labels'][0]:\n",
    "        # adding labels to the label list using the helper function\n",
    "        labels_list.append(labels_to_list(str([el['value']]))[0])\n",
    "    # saving the list of labels to the dictionary \n",
    "    met_labels[name] = labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3cc43-196e-4475-b252-86f65be3b107",
   "metadata": {},
   "source": [
    "### Appraisal Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c125dbe-77cb-4169-8af4-c67d9fed7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extractor(dic, txt):\n",
    "    '''\n",
    "    takes dic (a dictionary of dataframes) and txt (a string containing the label type to extract) as input\n",
    "    returns a dictionary with the same keys as dic, where the values are lists of label lists (e.g., [[1,7],[9,15]])\n",
    "    '''\n",
    "    # creating a dictionary of labels, where each key is a filename and each value is a list of labels\n",
    "    labels_dic = {}\n",
    "    # looping through all the keys and values in the input dictionary\n",
    "    for name, df in dic.items():\n",
    "        # creating an empty list of labels labels\n",
    "        labels_list = []\n",
    "        # dropping rows with no labels \n",
    "        cleaned_df = df[df.label != '_'][['indices', 'label']].dropna()\n",
    "        # if there are no labels, assigns an empty list\n",
    "        if len(cleaned_df.label) == 0:\n",
    "            labels_dic[name]=labels_list\n",
    "        else:\n",
    "            # extracting rows that contain the input txt in the label column\n",
    "            cleaned_df = cleaned_df[cleaned_df.label.str.contains(txt)]\n",
    "            # adding labels to the list of labels\n",
    "            for r in cleaned_df.indices:\n",
    "                labels_list.append([int(x) for x in r.split('-')])\n",
    "            labels_dic[name]=labels_list\n",
    "    return labels_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4f55ed-89d6-42d3-9dde-43b0c915ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOES NOT EXIST: aboriginal_17\n"
     ]
    }
   ],
   "source": [
    "# creating a dictionary of appraisal annotation dataframes, where each key is a filename\n",
    "appraisal_dict = {}\n",
    "\n",
    "# looping through each file name in the metaphor annotations \n",
    "for folder_id in met_df['filename'].unique():\n",
    "    path = 'SOCC/annotated/Appraisal/Appraisal_annotations/curation/' + folder_id\n",
    "    try:\n",
    "        # loading in appraisal annotations in folders that end with '.txt'\n",
    "        filename = os.listdir(path + '.txt')[0]\n",
    "        # reading and saving the annotations to a pandas dataframe\n",
    "        df = pd.read_csv(path + '.txt/' + filename, sep = '\\t', header = None, \n",
    "                         skiprows=6, names=['no.','indices','text','attitude','label','polarity'])\n",
    "        # saving the dataframe to the appraisal dictionary \n",
    "        appraisal_dict[folder_id] = df\n",
    "    except:\n",
    "        try:\n",
    "            # loading in appraisal annotations in folders that end with '.tsv'\n",
    "            filename = os.listdir(path + '.tsv')[0]\n",
    "            # reading and saving the annotations to a pandas dataframe\n",
    "            df = pd.read_csv(path + '.tsv/' + filename, sep = '\\t', header = None, \n",
    "            skiprows=6, names=['no.','indices','text','attitude','label','polarity'])\n",
    "            # saving the dataframe to the appraisal dictionary\n",
    "            appraisal_dict[folder_id] = df\n",
    "        except:\n",
    "            # prints the name of any file for which there is no appraisal annotation\n",
    "            print('DOES NOT EXIST:', folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53dde40b-8170-4ed1-bb80-a11299a400ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary where every key is a file name and every value is a list of the character indices in the negative labels \n",
    "neg_appr_labels = label_extractor(appraisal_dict, 'neg')\n",
    "# a dictionary where every key is a file name and every value is a list of the character indices in the positive labels \n",
    "pos_appr_labels = label_extractor(appraisal_dict, 'pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b100f-3495-4e7d-ba77-070ecc2af963",
   "metadata": {},
   "source": [
    "## Comparison \n",
    "### what are we measuring? how?\n",
    "We are deriving four metrics: (1) the percentage of metaphorical units that are labelled as positive, (2) the percentage of positive units that are labelled as metaphorical, (3) the percentage of metaphorical units that are labelled as negative, and (4) the percentage of negative units that are labelled as metaphorical. For (1), we first go through each label in the metaphor annotations, and check if 30% of the characters show up in a positive label. If they do, we say that that metaphorical unit is labelled as positive. We then calculate the percentage of positive metaphorical units by dividing the number of metaphors that are labelled as positive by the number of units labelled as metaphorical overall. The methodology is similar for (2)-(4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c191ee-28d0-47f5-a9fb-317659c3c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_calculator(dic1, dic2):\n",
    "    '''\n",
    "    finds the percentages of overlap between two dictionaries of labels and returns a dictionary where the values are percentages\n",
    "    '''\n",
    "    # setting counter for number of labels overall\n",
    "    dic1_total_labels, dic2_total_labels = 0, 0\n",
    "    \n",
    "    # number of labels in each dic that overlap (e.g., if dic1 has labels [1,3] and [4,9] for a file \n",
    "    # and dic2 has a label [1,9] for the same file, then dic1 has two overlapping labels with dic2 \n",
    "    # and dic2 has 1 overlapping label with dic1)\n",
    "    overall_overlap_cnt_dic1, overall_overlap_cnt_dic2 = 0, 0\n",
    "    \n",
    "    for file in list(dic1.keys()):\n",
    "        # initializing overlap count for each file in each dictionary; setting it to 0\n",
    "        overlap_cnt_dic1, overlap_cnt_dic2 = 0, 0\n",
    "\n",
    "        # loops through each label list in the list of list of labels corresponding to each key in the dictionary \n",
    "        for label_dic1 in dic1[file]:\n",
    "            # creating a set of the characters contained in the indices (e.g., [1,5] -> [1,2,3,4,5])\n",
    "            label_dic1_characters = set(list(range(label_dic1[0],label_dic1[1]+1)))\n",
    "            # loops through the labels in the second dictionary for the same key\n",
    "            for label_dic2 in dic2[file]:\n",
    "                # creating a set of the characters contained in the indices (e.g., [1,5] -> [1,2,3,4,5])\n",
    "                label_dic2_characters = set(list(range(label_dic2[0],label_dic2[1]+1)))\n",
    "                # variable representing the characters included in both labels (intersection)\n",
    "                overlap = label_dic1_characters & label_dic2_characters\n",
    "                # variable representing the elements included in either label (union)\n",
    "                universe = label_dic2_characters | label_dic2_characters\n",
    "                \n",
    "                # calculating overlap % if at least one element exists in both labels \n",
    "                if len(overlap) > 0:\n",
    "                    # the percentage of the first label that is included in the second label\n",
    "                    result_dic1 = float(len(overlap)) / len(label_dic1_characters) * 100\n",
    "                    # the percentage of the second label that is included in the first label\n",
    "                    result_dic2 = float(len(overlap)) / len(label_dic2_characters) * 100\n",
    "                    # if at least 30% of the first label is included in the second label, it counts as overlap\n",
    "                    if result_dic1 >= 30:\n",
    "                        # increasing counters by 1\n",
    "                        overlap_cnt_dic1+=1\n",
    "                        overall_overlap_cnt_dic1+=1\n",
    "                    # if at least 30% of the second label is included in the first label, it counts as overlap\n",
    "                    if result_dic2 >= 30:\n",
    "                        # increasing counters by 1\n",
    "                        overlap_cnt_dic2+=1\n",
    "                        overall_overlap_cnt_dic2+=1\n",
    "\n",
    "        # adding to counter of total labels for dic1\n",
    "        dic1_total_labels = dic1_total_labels + len(dic1[file])\n",
    "        # adding to counter of total labels for dic2\n",
    "        dic2_total_labels = dic2_total_labels + len(dic2[file])\n",
    "    return {'percentage of dic1 units that are labelled in dic2': overall_overlap_cnt_dic1/dic1_total_labels,\n",
    "            'percentage of dic2 units that are labelled in dic1': overall_overlap_cnt_dic2/dic2_total_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b51c93-5d48-41ea-b487-f8bc87e8da23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of metaphorical units that are labelled as positive: 0.07861635220125786\n",
      "percentage of positive units that are labelled as metaphors: 0.07496607869742199\n"
     ]
    }
   ],
   "source": [
    "overlap_percentage_pos = overlap_calculator(pos_appr_labels, met_labels).values()\n",
    "print('percentage of metaphorical units that are labelled as positive:', list(overlap_percentage_pos)[1])\n",
    "print('percentage of positive units that are labelled as metaphors:', list(overlap_percentage_pos)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26dc4202-7c2c-4528-9811-5068e85ec78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of metaphorical units that are labelled as negative: 0.30542452830188677\n",
      "percentage of negative units that are labelled as metaphors: 0.08239953632148377\n"
     ]
    }
   ],
   "source": [
    "overlap_percentage_neg = overlap_calculator(neg_appr_labels, met_labels).values()\n",
    "print('percentage of metaphorical units that are labelled as negative:', list(overlap_percentage_neg)[1])\n",
    "print('percentage of negative units that are labelled as metaphors:', list(overlap_percentage_neg)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a0b9f-8df3-4857-badd-1e43ba0aa333",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da61be64-3d7e-4702-9e3a-e9e116dcfc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hillary_5\n",
      "  indices    text          label\n",
      "7   39-45  damage  pos[2]|neg[3]\n",
      "hillary_45\n",
      "    indices        text          label\n",
      "89  428-438  destroying  pos[6]|neg[7]\n",
      "90  439-445      others  pos[6]|neg[7]\n",
      "91  446-448          to  pos[6]|neg[7]\n",
      "92  449-453        gain  pos[6]|neg[7]\n",
      "93  454-459       power  pos[6]|neg[7]\n",
      "94  460-464        over  pos[6]|neg[7]\n",
      "95  465-471      others  pos[6]|neg[7]\n",
      "hillary_80\n",
      "  indices       text          label\n",
      "8   44-53  illegally  pos[1]|neg[2]\n",
      "budget_18\n",
      "   indices  text          label\n",
      "18   78-82  good  neg[2]|pos[3]\n",
      "budget_22\n",
      "    indices    text          label\n",
      "24  134-136      in  neg[3]|pos[4]\n",
      "25  137-143  charge  neg[3]|pos[4]\n",
      "32  173-178   aware  neg[3]|pos[5]\n",
      "budget_25\n",
      "     indices     text           label\n",
      "39   178-185  awesome   neg[4]|pos[5]\n",
      "102  486-493  surplus  neg[9]|pos[11]\n",
      "107  517-521     good  neg[9]|pos[12]\n",
      "budget_34\n",
      "  indices    text          label\n",
      "8   50-56  fooled  pos[1]|neg[2]\n",
      "daycare_1\n",
      "    indices         text          label\n",
      "30  155-159         just  neg[2]|pos[3]\n",
      "31  160-171  established  neg[2]|pos[3]\n",
      "32  172-173            a  neg[2]|pos[3]\n",
      "33  174-182     national  neg[2]|pos[3]\n",
      "34  183-188        child  neg[2]|pos[3]\n",
      "35  189-193         care  neg[2]|pos[3]\n",
      "36  194-201      program  neg[2]|pos[3]\n",
      "38  203-206          not  neg[2]|pos[4]\n",
      "39  207-215     promised  neg[2]|pos[4]\n",
      "40  215-216            ,  neg[2]|pos[4]\n",
      "41  217-219           or  neg[2]|pos[4]\n",
      "42  220-228     planne4d  neg[2]|pos[4]\n",
      "43  228-229            ,  neg[2]|pos[4]\n",
      "44  230-233          but  neg[2]|pos[4]\n",
      "45  234-245  established  neg[2]|pos[4]\n",
      "daycare_13\n",
      "    indices         text          label\n",
      "37  202-213  interesting  neg[5]|pos[7]\n",
      "china_6\n",
      "  indices   text          label\n",
      "7   42-47  peace  neg[2]|pos[3]\n",
      "pope_17\n",
      "    indices           text          label\n",
      "26  138-151  authoritarian  pos[3]|neg[4]\n",
      "trump_9\n",
      "    indices    text          label\n",
      "40  177-183  enmity  pos[2]|neg[3]\n",
      "aboriginal_2\n",
      "   indices    text          label\n",
      "20   85-91  thrive  neg[1]|pos[2]\n",
      "aboriginal_8\n",
      "   indices       text          label\n",
      "4    21-29   Violence  pos[2]|neg[3]\n",
      "5    30-37    against  pos[2]|neg[3]\n",
      "6    38-43      women  pos[2]|neg[3]\n",
      "7    44-47        and  pos[2]|neg[3]\n",
      "8    48-56   children  pos[2]|neg[3]\n",
      "16  92-101  addiction  pos[4]|neg[5]\n",
      "aboriginal_10\n",
      "       indices   text                    label\n",
      "210   998-1003  awful  pos[16]|pos[17]|neg[18]\n",
      "211  1004-1009  abuse  pos[16]|pos[17]|neg[18]\n",
      "aboriginal_28\n",
      "  indices         text          label\n",
      "7   45-56  residential  pos[1]|neg[2]\n",
      "8   57-64      schools  pos[1]|neg[2]\n",
      "aboriginal_30\n",
      "    indices      text          label\n",
      "5     44-52  muddying  pos[2]|neg[3]\n",
      "6     53-56       the  pos[2]|neg[3]\n",
      "7     57-63    waters  pos[2]|neg[3]\n",
      "10    78-85   poverty  pos[2]|neg[4]\n",
      "11    86-89       and  pos[2]|neg[4]\n",
      "12    90-96    social  pos[2]|neg[4]\n",
      "13   97-105  problems  pos[2]|neg[4]\n",
      "18  124-131   violent  pos[2]|neg[5]\n",
      "19  132-137     crime  pos[2]|neg[5]\n",
      "aboriginal_50\n",
      "    indices      text          label\n",
      "35  163-168     grown  neg[6]|pos[7]\n",
      "36  168-169         ,  neg[6]|pos[7]\n",
      "37  170-178  educated  neg[6]|pos[7]\n",
      "38  179-182       and  neg[6]|pos[7]\n",
      "39  183-191  employed  neg[6]|pos[7]\n",
      "aboriginal_57\n",
      "    indices            text          label\n",
      "44  243-247            pony  neg[6]|pos[7]\n",
      "45  248-251             ing  neg[6]|pos[7]\n",
      "46  252-254              up  neg[6]|pos[7]\n",
      "47  255-258             and  neg[6]|pos[7]\n",
      "48  259-268       accepting  neg[6]|pos[7]\n",
      "49  269-283  responsibility  neg[6]|pos[7]\n",
      "belgium_22\n",
      "  indices        text          label\n",
      "4   29-39  productive  neg[1]|pos[2]\n",
      "belgium_43\n",
      "  indices     text          label\n",
      "4   33-40  honesty  neg[1]|pos[2]\n",
      "belgium_55\n",
      "    indices       text          label\n",
      "25  143-150    suicide  pos[2]|neg[3]\n",
      "31  182-191  martyrdom  pos[2]|neg[4]\n",
      "belgium_62\n",
      "    indices     text          label\n",
      "66  412-419  rescues  neg[6]|pos[7]\n",
      "belgium_75\n",
      "    indices   text          label\n",
      "59  345-350  happy  neg[7]|pos[8]\n",
      "belgium_77\n",
      "    indices       text          label\n",
      "51  245-254  attacking  pos[2]|neg[3]\n",
      "52  254-255          ,  pos[2]|neg[3]\n",
      "53  256-264   shooting  pos[2]|neg[3]\n",
      "54  264-265          ,  pos[2]|neg[3]\n",
      "55  266-273    suicide  pos[2]|neg[3]\n",
      "56  274-281    bombing  pos[2]|neg[3]\n",
      "57  282-285        and  pos[2]|neg[3]\n",
      "58  286-293    killing  pos[2]|neg[3]\n",
      "belgium_91\n",
      "    indices      text          label\n",
      "54  289-295    agency  neg[7]|pos[8]\n",
      "55  296-299       and  neg[7]|pos[8]\n",
      "56  300-308  autonomy  neg[7]|pos[8]\n",
      "belgium_103\n",
      "    indices       text          label\n",
      "14    78-87  terrorism  pos[2]|neg[3]\n",
      "27  161-170  terrorism  pos[4]|neg[5]\n",
      "belgium_112\n",
      "       indices       text            label\n",
      "18      91-100  apartheid    pos[4]|neg[5]\n",
      "243  1199-1206    radical  pos[36]|neg[37]\n",
      "budget_123\n",
      "     indices       text          label\n",
      "110  495-504  democracy  neg[6]|pos[7]\n",
      "hillary_126\n",
      "    indices     text                 label\n",
      "11    52-56     went         neg[1]|pos[2]\n",
      "12    57-61     with         neg[1]|pos[2]\n",
      "13    62-67    Trump         neg[1]|pos[2]\n",
      "14    68-71      and         neg[1]|pos[2]\n",
      "15    72-78   turned         neg[1]|pos[2]\n",
      "16    79-83     down         neg[1]|pos[2]\n",
      "17    84-87      the         neg[1]|pos[2]\n",
      "18    88-94   option         neg[1]|pos[2]\n",
      "19    95-97       to         neg[1]|pos[2]\n",
      "20   98-102     back         neg[1]|pos[2]\n",
      "21  103-110  Billary  neg[1]|pos[2]|neg[3]\n",
      "property_29\n",
      "    indices    text          label\n",
      "19  108-114  abuses  pos[1]|neg[2]\n",
      "property_31\n",
      "   indices  text          label\n",
      "11   56-60  good  neg[1]|pos[2]\n",
      "property_60\n",
      "     indices     text          label\n",
      "115  541-548  imposed  neg[6]|pos[7]\n",
      "116  549-550        a  neg[6]|pos[7]\n",
      "117  551-558  similar  neg[6]|pos[7]\n",
      "118  559-562      tax  neg[6]|pos[7]\n",
      "property_75\n",
      "    indices       text          label\n",
      "27  146-153    totally  neg[1]|pos[4]\n",
      "28  154-163  avoidable  neg[1]|pos[4]\n",
      "property_81\n",
      "  indices     text          label\n",
      "7   48-55  problem  pos[1]|neg[2]\n",
      "property_90\n",
      "  indices    text          label\n",
      "2   14-20  honest  neg[1]|pos[2]\n",
      "property_94\n",
      "    indices     text          label\n",
      "32  185-190    using  pos[2]|neg[3]\n",
      "33  191-194      our  pos[2]|neg[3]\n",
      "34  195-202  economy  pos[2]|neg[3]\n",
      "35  203-205       as  pos[2]|neg[3]\n",
      "36  206-207        a  pos[2]|neg[3]\n",
      "37  208-213    cheap  pos[2]|neg[3]\n",
      "38  214-218     bank  pos[2]|neg[3]\n",
      "refugees_20\n",
      "    indices     text          label\n",
      "15    83-90  helping  neg[2]|pos[3]\n",
      "16    91-92        a  neg[2]|pos[3]\n",
      "17    93-99   fellow  neg[2]|pos[3]\n",
      "18  100-105    human  neg[2]|pos[3]\n",
      "19  106-111    being  neg[2]|pos[3]\n",
      "refugees_54\n",
      "    indices     text          label\n",
      "21   99-103     rich  neg[3]|pos[4]\n",
      "34  147-154  problem  pos[5]|neg[6]\n",
      "refugees_69\n",
      "    indices      text                 label\n",
      "10    64-67       war  pos[1]|pos[2]|neg[3]\n",
      "19  108-115   tsunami         pos[4]|neg[5]\n",
      "20  116-118        of         pos[4]|neg[5]\n",
      "21  119-127  refugees         pos[4]|neg[5]\n",
      "trump_33\n",
      "    indices         text          label\n",
      "3     25-32      extreme  pos[2]|neg[3]\n",
      "20  104-115  constrained  neg[6]|pos[7]\n",
      "trump_57\n",
      "    indices        text          label\n",
      "70  318-322        felt  pos[4]|neg[5]\n",
      "71  323-324           a  pos[4]|neg[5]\n",
      "72  325-333    distinct  pos[4]|neg[5]\n",
      "73  334-344  disconnect  pos[4]|neg[5]\n",
      "trump_71\n",
      "       indices        text                 label\n",
      "53     299-309  meaningful         neg[7]|pos[8]\n",
      "54     309-310           ,         neg[7]|pos[8]\n",
      "55     311-320   sometimes         neg[7]|pos[8]\n",
      "56     321-328     painful  neg[7]|pos[8]|neg[9]\n",
      "57     329-332         but         neg[7]|pos[8]\n",
      "58     333-342   necessary         neg[7]|pos[8]\n",
      "180  1021-1029    economic       neg[21]|pos[22]\n",
      "181  1030-1035       giant       neg[21]|pos[22]\n",
      "trump_76\n",
      "   indices       text          label\n",
      "16  96-105  socialism  pos[2]|neg[3]\n",
      "trump_93\n",
      "  indices       text          label\n",
      "3   24-33  political  pos[1]|neg[2]\n",
      "4   34-42   trifecta  pos[1]|neg[2]\n",
      "trump_114\n",
      "   indices  text          label\n",
      "11   54-58  lies  pos[2]|neg[3]\n",
      "trump_115\n",
      "    indices    text          label\n",
      "93  495-501  afraid  pos[5]|neg[6]\n",
      "trump_126\n",
      "    indices   text          label\n",
      "42  175-180  truth  neg[4]|pos[5]\n",
      "48  199-203   good  neg[4]|pos[6]\n",
      "uber_10\n",
      "  indices       text          label\n",
      "7   35-39       fair  neg[1]|pos[2]\n",
      "8   40-43        and  neg[1]|pos[2]\n",
      "9   44-53  civilized  neg[1]|pos[2]\n",
      "uber_18\n",
      "    indices  text            label\n",
      "78  334-338  fair  neg[10]|pos[11]\n",
      "uber_27\n",
      "    indices  text            label\n",
      "66  273-277  true  neg[10]|pos[11]\n",
      "uber_41\n",
      "   indices  text          label\n",
      "16   60-64  real  neg[1]|pos[2]\n",
      "uber_45\n",
      "    indices  text          label\n",
      "49  280-284  want  neg[8]|pos[9]\n",
      "uber_48\n",
      "    indices    text           label\n",
      "69  342-345     new  neg[9]|pos[11]\n",
      "70  346-349     and  neg[9]|pos[11]\n",
      "71  350-352      in  neg[9]|pos[11]\n",
      "72  353-359  proved  neg[9]|pos[11]\n",
      "73  360-363     way  neg[9]|pos[11]\n",
      "uber_53\n",
      "  indices         text          label\n",
      "2   12-23  competition  neg[1]|pos[2]\n",
      "uber_56\n",
      "    indices      text          label\n",
      "25  108-116  solution  pos[2]|neg[3]\n",
      "uber_62\n",
      "       indices       text            label\n",
      "258  1209-1218  middlemen  pos[17]|neg[18]\n",
      "uber_76\n",
      "    indices       text          label\n",
      "43  241-250  collusion  pos[8]|neg[9]\n",
      "uber_90\n",
      "    indices  text          label\n",
      "44  240-244  just  neg[5]|pos[6]\n",
      "uber_108\n",
      "   indices  text          label\n",
      "15   86-89   not  neg[2]|pos[3]\n",
      "16   90-92    so  neg[2]|pos[3]\n",
      "17   93-97  hard  neg[2]|pos[3]\n",
      "uber_114\n",
      "  indices   text          label\n",
      "6   27-32  worry  pos[2]|neg[3]\n",
      "uber_116\n",
      "    indices  text          label\n",
      "53  234-238  good  neg[8]|pos[9]\n",
      "watch_55\n",
      "     indices      text          label\n",
      "110  466-471     still  pos[5]|neg[6]\n",
      "111  472-480  reaching  pos[5]|neg[6]\n",
      "112  481-484       for  pos[5]|neg[6]\n",
      "113  485-490     their  pos[5]|neg[6]\n",
      "114  491-496     phone  pos[5]|neg[6]\n",
      "Aboriginal_43\n",
      "   indices  text          label\n",
      "11   56-60  real  neg[2]|pos[3]\n",
      "Belgium_100\n",
      "    indices      text          label\n",
      "48  272-280  moderate  neg[4]|pos[5]\n",
      "Belgium_116\n",
      "    indices        text          label\n",
      "27  123-133  terrorists  pos[2]|neg[3]\n",
      "Uber_82\n",
      "    indices   text          label\n",
      "19   96-100   real  neg[2]|pos[3]\n",
      "20  101-106  truth  neg[2]|pos[3]\n"
     ]
    }
   ],
   "source": [
    "# checking which units contain both a positive and a negative label\n",
    "for name, df in appraisal_dict.items():\n",
    "    cleaned_df = df[df.label != '_'][['indices', 'text', 'label']].dropna()\n",
    "    if len(cleaned_df) > 0:\n",
    "        neg_df = cleaned_df[cleaned_df.label.str.contains('neg')]\n",
    "        pos_neg = neg_df[neg_df.label.str.contains('pos')]\n",
    "        if len(pos_neg) > 0:\n",
    "            print(name)\n",
    "            print(pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5729de-5979-4428-9516-480d59ab1c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
