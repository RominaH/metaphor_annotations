{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POcTijMFrlD2"
   },
   "source": [
    "# Script for comparing annotation\n",
    "Currently, we are looking to compare the annotations between annotators regarding metaphors and non-metaphors. Note that all words/phrases that are tagged are only tagged with the label [METAPHOR]. We want a script that:\n",
    "1. Compares words/phases that get tagged\n",
    "2. Compares the tags associated with these words/phrases\n",
    "3. Outputs some sort of agreement level between annotators\n",
    "\n",
    "In the future, we also want some way to compare these annotations with the appraisal annotations for these articles to see what class of words tend to be classified as metaphors or non-metaphors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VxO9-urYrY4l"
   },
   "outputs": [],
   "source": [
    "# # run this block if connecting to google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HehCnqwNvDD0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statistics import mean\n",
    "from ast import literal_eval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo3IsnQMsQ9n"
   },
   "source": [
    "## Load annotations from LabelStudio output\n",
    "LabelStudio can output the annotations in various formats, including json, csv, and tsv.\n",
    "\n",
    "For simplicity we are using the csv option, which includes the following columns:\n",
    "* **annotation_id** - unique id associated with each annotation\n",
    "* **annotator** - id associated with a particular annotator\n",
    "* **created_at** - time the annotation was created\n",
    "* **id** - text id\n",
    "* **label** - dictionary of labels associated with the text\n",
    "  * \"start\": the index of the first labelled character within the selected text\n",
    "  * \"end\": the index of the final labelled character within the selected text\n",
    "  * \"text\": the text (word/phrase) that was selected to be labelled\n",
    "  * \"labels\": a list of labels associated with the selected text\n",
    "* **lead_time** - (not sure what this is)\n",
    "* **text** - article text\n",
    "* **updated_at** - time the annotation was last updated\n",
    "\n",
    "Relevant columns here are annotator, id (rename to text_id), label, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\romha\\\\OneDrive - University of Waterloo\\\\Desktop\\\\Spr25\\\\metaphor_annotations'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yw4fSAB_u7sg"
   },
   "outputs": [],
   "source": [
    "input = 'mar4.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>text</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-01-25T03:02:14.469802Z</td>\n",
       "      <td>1</td>\n",
       "      <td>[{\"start\":151,\"end\":162,\"text\":\"blue collar\",\"...</td>\n",
       "      <td>9.888</td>\n",
       "      <td>' The problem is bigger than Mr. Trump.'Extend...</td>\n",
       "      <td>2025-01-25T03:02:14.469842Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-01-25T03:02:37.974849Z</td>\n",
       "      <td>2</td>\n",
       "      <td>[{\"start\":298,\"end\":306,\"text\":\"genitals\",\"lab...</td>\n",
       "      <td>28.824</td>\n",
       "      <td>'If she was a man, she would be president-elec...</td>\n",
       "      <td>2025-02-06T17:15:28.502235Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-01-25T02:48:07.557501Z</td>\n",
       "      <td>3</td>\n",
       "      <td>[{\"start\":140,\"end\":146,\"text\":\"engage\",\"label...</td>\n",
       "      <td>176.665</td>\n",
       "      <td>'What are you going to tell your daughters?'Te...</td>\n",
       "      <td>2025-02-06T17:16:36.503806Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>511</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-01-25T03:04:01.361546Z</td>\n",
       "      <td>4</td>\n",
       "      <td>[{\"start\":313,\"end\":316,\"text\":\"top\",\"labels\":...</td>\n",
       "      <td>101.213</td>\n",
       "      <td>'What are you going to tell your daughters?'We...</td>\n",
       "      <td>2025-02-01T06:28:56.983552Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>503</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-01-25T02:48:59.759517Z</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.503</td>\n",
       "      <td>A PR piece designed to reverse all the damage ...</td>\n",
       "      <td>2025-01-25T02:48:59.759572Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1133</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-02-05T01:02:28.018384Z</td>\n",
       "      <td>1142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.526</td>\n",
       "      <td>The license has nothing to do with almost any ...</td>\n",
       "      <td>2025-02-05T01:02:28.018427Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1132</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-02-05T01:01:59.522189Z</td>\n",
       "      <td>1143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.664</td>\n",
       "      <td>They dictate meter rates because, as the artic...</td>\n",
       "      <td>2025-02-05T01:01:59.522227Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1131</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-02-05T00:06:40.287424Z</td>\n",
       "      <td>1144</td>\n",
       "      <td>[{\"start\":138,\"end\":142,\"text\":\"pays\",\"labels\"...</td>\n",
       "      <td>1907.271</td>\n",
       "      <td>UBER customers just want a CHEAPER ride (no in...</td>\n",
       "      <td>2025-02-05T00:07:08.794118Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1130</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-02-04T23:35:06.074462Z</td>\n",
       "      <td>1145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>236.592</td>\n",
       "      <td>Why? The taxi medallions allow a cartel to fun...</td>\n",
       "      <td>2025-02-04T23:35:06.074500Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1129</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-02-04T23:30:49.722084Z</td>\n",
       "      <td>1146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.866</td>\n",
       "      <td>tempsperdue  13 minutes ago At most the city s...</td>\n",
       "      <td>2025-02-04T23:30:49.722120Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1015 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      annotation_id  annotator                   created_at    id  \\\n",
       "0               509          4  2025-01-25T03:02:14.469802Z     1   \n",
       "1               510          4  2025-01-25T03:02:37.974849Z     2   \n",
       "2               502          4  2025-01-25T02:48:07.557501Z     3   \n",
       "3               511          4  2025-01-25T03:04:01.361546Z     4   \n",
       "4               503          4  2025-01-25T02:48:59.759517Z     5   \n",
       "...             ...        ...                          ...   ...   \n",
       "1010           1133          3  2025-02-05T01:02:28.018384Z  1142   \n",
       "1011           1132          3  2025-02-05T01:01:59.522189Z  1143   \n",
       "1012           1131          3  2025-02-05T00:06:40.287424Z  1144   \n",
       "1013           1130          3  2025-02-04T23:35:06.074462Z  1145   \n",
       "1014           1129          3  2025-02-04T23:30:49.722084Z  1146   \n",
       "\n",
       "                                                  label  lead_time  \\\n",
       "0     [{\"start\":151,\"end\":162,\"text\":\"blue collar\",\"...      9.888   \n",
       "1     [{\"start\":298,\"end\":306,\"text\":\"genitals\",\"lab...     28.824   \n",
       "2     [{\"start\":140,\"end\":146,\"text\":\"engage\",\"label...    176.665   \n",
       "3     [{\"start\":313,\"end\":316,\"text\":\"top\",\"labels\":...    101.213   \n",
       "4                                                   NaN      1.503   \n",
       "...                                                 ...        ...   \n",
       "1010                                                NaN     20.526   \n",
       "1011                                                NaN     71.664   \n",
       "1012  [{\"start\":138,\"end\":142,\"text\":\"pays\",\"labels\"...   1907.271   \n",
       "1013                                                NaN    236.592   \n",
       "1014                                                NaN     44.866   \n",
       "\n",
       "                                                   text  \\\n",
       "0     ' The problem is bigger than Mr. Trump.'Extend...   \n",
       "1     'If she was a man, she would be president-elec...   \n",
       "2     'What are you going to tell your daughters?'Te...   \n",
       "3     'What are you going to tell your daughters?'We...   \n",
       "4     A PR piece designed to reverse all the damage ...   \n",
       "...                                                 ...   \n",
       "1010  The license has nothing to do with almost any ...   \n",
       "1011  They dictate meter rates because, as the artic...   \n",
       "1012  UBER customers just want a CHEAPER ride (no in...   \n",
       "1013  Why? The taxi medallions allow a cartel to fun...   \n",
       "1014  tempsperdue  13 minutes ago At most the city s...   \n",
       "\n",
       "                       updated_at  \n",
       "0     2025-01-25T03:02:14.469842Z  \n",
       "1     2025-02-06T17:15:28.502235Z  \n",
       "2     2025-02-06T17:16:36.503806Z  \n",
       "3     2025-02-01T06:28:56.983552Z  \n",
       "4     2025-01-25T02:48:59.759572Z  \n",
       "...                           ...  \n",
       "1010  2025-02-05T01:02:28.018427Z  \n",
       "1011  2025-02-05T01:01:59.522227Z  \n",
       "1012  2025-02-05T00:07:08.794118Z  \n",
       "1013  2025-02-04T23:35:06.074500Z  \n",
       "1014  2025-02-04T23:30:49.722120Z  \n",
       "\n",
       "[1015 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = [11, 72, 131, 146, 178, 185, 214, 235, 258, 280, 333, \n",
    "            339, 414, 420, 495, 504, 521, 532, 593, 1139, \n",
    "            606, 678, 683, 745, 751, 762, 853, 864, 907, 908]\n",
    "len(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-03-02T22:59:31.345865Z</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another load of tosh from a GTA Liberal</td>\n",
       "      <td>2025-03-03T01:39:11.709897Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-02T22:28:24.168230Z</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another load of tosh from a GTA Liberal</td>\n",
       "      <td>2025-03-02T23:01:27.466009Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-01-25T03:05:57.967446Z</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another load of tosh from a GTA Liberal</td>\n",
       "      <td>2025-01-25T03:05:57.967512Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-03-03T01:27:45.721978Z</td>\n",
       "      <td>72</td>\n",
       "      <td>[{\"start\":38,\"end\":43,\"text\":\"holds\",\"labels\":...</td>\n",
       "      <td>Nobody really cares if a woman or man holds th...</td>\n",
       "      <td>2025-03-03T01:39:56.571135Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-02T22:29:08.419293Z</td>\n",
       "      <td>72</td>\n",
       "      <td>[{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels...</td>\n",
       "      <td>Nobody really cares if a woman or man holds th...</td>\n",
       "      <td>2025-03-02T23:26:05.507051Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    annotator                   created_at  id  \\\n",
       "10          3  2025-03-02T22:59:31.345865Z  11   \n",
       "11          5  2025-03-02T22:28:24.168230Z  11   \n",
       "12          4  2025-01-25T03:05:57.967446Z  11   \n",
       "73          3  2025-03-03T01:27:45.721978Z  72   \n",
       "74          5  2025-03-02T22:29:08.419293Z  72   \n",
       "\n",
       "                                                label  \\\n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "73  [{\"start\":38,\"end\":43,\"text\":\"holds\",\"labels\":...   \n",
       "74  [{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels...   \n",
       "\n",
       "                                                 text  \\\n",
       "10            Another load of tosh from a GTA Liberal   \n",
       "11            Another load of tosh from a GTA Liberal   \n",
       "12            Another load of tosh from a GTA Liberal   \n",
       "73  Nobody really cares if a woman or man holds th...   \n",
       "74  Nobody really cares if a woman or man holds th...   \n",
       "\n",
       "                     updated_at  \n",
       "10  2025-03-03T01:39:11.709897Z  \n",
       "11  2025-03-02T23:01:27.466009Z  \n",
       "12  2025-01-25T03:05:57.967512Z  \n",
       "73  2025-03-03T01:39:56.571135Z  \n",
       "74  2025-03-02T23:26:05.507051Z  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df = pd.read_csv(input, usecols=[\"annotator\", \"id\", \"label\", \"text\", \"created_at\", \"updated_at\"])\n",
    "# annotations_df=[\"updated_at\"]>\"2025-02-25\"][[\"annotator\", \"id\", \"label\", \"text\"]]\n",
    "annotations_df=annotations_df[annotations_df['id'].isin(selected)]\n",
    "annotations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9IpFkjpcvMfo"
   },
   "outputs": [],
   "source": [
    "# rename 'id' to 'text_id' for clarification\n",
    "annotations_df.rename(columns={\"id\": \"text_id\"}, inplace=True)\n",
    "\n",
    "# reorder columns for clarification\n",
    "order = [\"text_id\", \"annotator\", \"text\", \"label\"]\n",
    "annotations_df = annotations_df.reindex(columns=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1QJkGvitvfuP",
    "outputId": "79d0aa41-52ac-408d-9287-6c9558e5fae3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>Another load of tosh from a GTA Liberal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>Another load of tosh from a GTA Liberal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>Another load of tosh from a GTA Liberal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>Nobody really cares if a woman or man holds th...</td>\n",
       "      <td>[{\"start\":38,\"end\":43,\"text\":\"holds\",\"labels\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>Nobody really cares if a woman or man holds th...</td>\n",
       "      <td>[{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text_id  annotator                                               text  \\\n",
       "10       11          3            Another load of tosh from a GTA Liberal   \n",
       "11       11          5            Another load of tosh from a GTA Liberal   \n",
       "12       11          4            Another load of tosh from a GTA Liberal   \n",
       "73       72          3  Nobody really cares if a woman or man holds th...   \n",
       "74       72          5  Nobody really cares if a woman or man holds th...   \n",
       "\n",
       "                                                label  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "73  [{\"start\":38,\"end\":43,\"text\":\"holds\",\"labels\":...  \n",
       "74  [{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUt6XBcqyyS7",
    "outputId": "816bce6e-509a-4aa1-d472-bb2f70f6326c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all the unique annotators\n",
    "annotations_df['annotator'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-9dkFOJzO6I"
   },
   "source": [
    "## Comparing tagged words and labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeNaZDYcAvQi"
   },
   "source": [
    "Tentative idea for how to proceed:\n",
    "* With just two annotators, we could manually split the annotations into two dataframes, and then merge them again on text id, giving us a new dataframe containing the text id, the article text, and two label columns from the annotators\n",
    "* From there, we can write a function to compare the lists of dictionaries in the label columns, and then create the following additional columns in this new merged dataframe:\n",
    "  * overlap_agreement - if there is an overlap between non-identical tags (e.g. \"line\" vs \"line in the sand\"), how much do they agree?\n",
    "  * textual_agreement - of words/phrases tagged, how many are identical?\n",
    "\n",
    "To filter by annotators, use the numbers found from the previous code block and replace them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7KW0szom5mKk"
   },
   "outputs": [],
   "source": [
    "# filter the dataframe to contain just copies of annotations from each annotator\n",
    "annotator1_df = annotations_df.loc[(annotations_df['annotator'] == 4)] # romina\n",
    "annotator2_df = annotations_df.loc[(annotations_df['annotator'] == 5)] # amber\n",
    "annotator3_df = annotations_df.loc[(annotations_df['annotator'] == 3)] # vanja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1F3XCbRkDY-y",
    "outputId": "8fad364d-4374-4c7f-e274-c214bc46e2c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>Another load of tosh from a GTA Liberal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>Nobody really cares if a woman or man holds th...</td>\n",
       "      <td>[{\"start\":322,\"end\":326,\"text\":\"bill\",\"labels\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>131</td>\n",
       "      <td>4</td>\n",
       "      <td>Funnily enough, if the Ontario Liberals tried ...</td>\n",
       "      <td>[{\"start\":48,\"end\":58,\"text\":\"shell game\",\"lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>146</td>\n",
       "      <td>4</td>\n",
       "      <td>Apple stuff consist of 95% marketing nonsense ...</td>\n",
       "      <td>[{\"start\":138,\"end\":144,\"text\":\"thrive\",\"label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>This whole concept of 'national daycare' sound...</td>\n",
       "      <td>[{\"start\":72,\"end\":76,\"text\":\"he##\",\"labels\":[...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text_id  annotator                                               text  \\\n",
       "12        11          4            Another load of tosh from a GTA Liberal   \n",
       "75        72          4  Nobody really cares if a woman or man holds th...   \n",
       "136      131          4  Funnily enough, if the Ontario Liberals tried ...   \n",
       "153      146          4  Apple stuff consist of 95% marketing nonsense ...   \n",
       "187      178          4  This whole concept of 'national daycare' sound...   \n",
       "\n",
       "                                                 label  \n",
       "12                                                 NaN  \n",
       "75   [{\"start\":322,\"end\":326,\"text\":\"bill\",\"labels\"...  \n",
       "136  [{\"start\":48,\"end\":58,\"text\":\"shell game\",\"lab...  \n",
       "153  [{\"start\":138,\"end\":144,\"text\":\"thrive\",\"label...  \n",
       "187  [{\"start\":72,\"end\":76,\"text\":\"he##\",\"labels\":[...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotator1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NWIsCvHFDaln",
    "outputId": "907833ca-5da3-4ea7-f9e1-f6bc172277e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>Another load of tosh from a GTA Liberal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>Nobody really cares if a woman or man holds th...</td>\n",
       "      <td>[{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>131</td>\n",
       "      <td>5</td>\n",
       "      <td>Funnily enough, if the Ontario Liberals tried ...</td>\n",
       "      <td>[{\"start\":48,\"end\":58,\"text\":\"shell game\",\"lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>146</td>\n",
       "      <td>5</td>\n",
       "      <td>Apple stuff consist of 95% marketing nonsense ...</td>\n",
       "      <td>[{\"start\":125,\"end\":130,\"text\":\"knows\",\"labels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>178</td>\n",
       "      <td>5</td>\n",
       "      <td>This whole concept of 'national daycare' sound...</td>\n",
       "      <td>[{\"start\":294,\"end\":302,\"text\":\"draining\",\"lab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text_id  annotator                                               text  \\\n",
       "11        11          5            Another load of tosh from a GTA Liberal   \n",
       "74        72          5  Nobody really cares if a woman or man holds th...   \n",
       "135      131          5  Funnily enough, if the Ontario Liberals tried ...   \n",
       "152      146          5  Apple stuff consist of 95% marketing nonsense ...   \n",
       "186      178          5  This whole concept of 'national daycare' sound...   \n",
       "\n",
       "                                                 label  \n",
       "11                                                 NaN  \n",
       "74   [{\"start\":48,\"end\":55,\"text\":\"highest\",\"labels...  \n",
       "135  [{\"start\":48,\"end\":58,\"text\":\"shell game\",\"lab...  \n",
       "152  [{\"start\":125,\"end\":130,\"text\":\"knows\",\"labels...  \n",
       "186  [{\"start\":294,\"end\":302,\"text\":\"draining\",\"lab...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotator2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-kdNtqGVZCy"
   },
   "source": [
    "Here we create a few helper functions to turn the label columns into useable data.\n",
    "\n",
    "First, `labels_to_list` takes in the labels columns and returns a list of lists containing just the start and end indices of tagged words/phrases. These uniquely identify a tagged item within the text, and disambiguates between different instances of the same word (e.g. if \"line\" appears multiple times throughout the comment), so it is all the information we will need to work with.\n",
    "\n",
    "Each item in the returned list is a list of [start index, end index pairs], extracted from the list of dictionaries of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Z1s0ugHq4IBp"
   },
   "outputs": [],
   "source": [
    "# helper function to build a list of lists containing the start and end indices\n",
    "# and the difference between these indices\n",
    "# where labels is the labels associated with a specific text\n",
    "def labels_to_list(labels):\n",
    "  annotations = []\n",
    "\n",
    "  labels = literal_eval(labels)\n",
    "\n",
    "  for label in labels:\n",
    "    tags = []\n",
    "    tags.append(int(label['start']))\n",
    "    tags.append(int(label['end']))\n",
    "\n",
    "    annotations.append(tags)\n",
    "\n",
    "  return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwgczKeuV-SS"
   },
   "source": [
    "This is the main helper function that will produce textual and overlap agreement. As a reminder, textual agreement refers to the similarity in items that are tagged between the annotators (we are not counting the number of tags, but rather, the specific words that are tagged - it would not be very useful to know that they share the same number of annotations if they are both tagging different words). Overlap agreement looks specifically at items that may overlap, for example, if one annotator only tagged \"line\" as metaphor but the other tagged the entire phrase \"line in the sand\".\n",
    "\n",
    "To determine the textual agreement, we first make the labels dictionaries into usable start-end index pairs, making sure to sort them in order (the csv output seems to list the labels in the order that they are created, not in the order the text appears in the comment). Then, we iterate through the two lists to create two additional lists - these contain the annotations that will later be used to compare against each other to produce cohen's kappa.\n",
    "\n",
    "To produce the additional lists, we add in order the labels that appear in both lists; when a label only appears in one list, we append \"0, 0\" to the other list to denote that the item was not annotated by that annotator.\n",
    "\n",
    "It is trickier to use cohen's kappa to determine overlap agreement. As such, I used a rudimentary/brute force method where for each overlaping (but, crucially, not identical) label, I find the ratio between the difference of the indices and add this to a third list for easier calculation.\n",
    "\n",
    "As an example: let's say \"line\" has indices [0, 3] and \"line in the sand\" has indices [0, 15]. Taking the difference, \"line\" has a difference of `3` and \"line in the sand\" has a differnece of `15`. We then divide `3/15` to get `0.2`, telling us that the two tags only overlap by `20%`.\n",
    "\n",
    "Once these ratios are calculated, we then find the mean of these overlap ratios to determine how closely the overlapping tags were to each other, with 0 being no overlap at all and 1 being 100% overlap (note: a 1 should never appear, as for comments where there is no overlap between annotations, `NaN` is instead appended to the row to avoid messing up final calculations).\n",
    "\n",
    "Similarly, a 1 in textual agreement suggests 100% agreement, and a 0 indicates 0% agreement between the annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eeKdIk7ybmWF"
   },
   "outputs": [],
   "source": [
    "# helper function to compare tags and labels for a single text\n",
    "# where row contains the text and labels from both annotators associated with the text\n",
    "# label1 and label2 are the column headers for the labels\n",
    "# returns a list containing the agreement score\n",
    "# of overlapping labels and overall tags\n",
    "def compare_labels(row, label1, label2):\n",
    "\n",
    "  # turn labels dictionaries into usable lists\n",
    "  list1 = labels_to_list(row[label1])\n",
    "  list2 = labels_to_list(row[label2])\n",
    "\n",
    "  # sort the keys to be in index order\n",
    "  # since LabelStudio orders it by time updated, not order of appearance in text\n",
    "  list1 = sorted(list1, key=lambda x: x[0])\n",
    "  list2 = sorted(list2, key=lambda x: x[0])\n",
    "\n",
    "  tags1 = []\n",
    "  tags2 = []\n",
    "  overlap = []\n",
    "\n",
    "  i = 0\n",
    "  j = 0\n",
    "\n",
    "  # cohen's kappa takes two lists of equal lengths to compare labels\n",
    "  # so here we are building new lists from the annotations\n",
    "  # appending 0's where the label exists in one list but not the other\n",
    "  # and iterating through both lists until we have read both of them in full\n",
    "\n",
    "  while i < len(list1) and j < len(list2):\n",
    "    # if the current label in list1 appears before the current label in list2\n",
    "    # append list1's label to tags1\n",
    "    # and append the '0, 0' label to tags2\n",
    "    if list1[i][0] < list2[j][0]:\n",
    "      tags1.append(str(list1[i]))\n",
    "      tags2.append(\"0, 0\")\n",
    "      i += 1\n",
    "\n",
    "    # if the current label in list 1 starts with the same word instance\n",
    "    # as the label in list2\n",
    "    # append both labels to their respective lists\n",
    "    elif list1[i][0] == list2[j][0]:\n",
    "      tags1.append(str(list1[i]))\n",
    "      tags2.append(str(list2[j]))\n",
    "\n",
    "      # and here we check for overlap with the difference in index\n",
    "      # e.g. if one label is [4, 10] and the other [4, 16]\n",
    "      # we compare the difference of 6 and 12\n",
    "      # and append 6/12, or 0.5, to the overlap list\n",
    "      if list1[i][1] != list2[j][1]:\n",
    "        diff1 = list1[i][1] - list1[i][0]\n",
    "        diff2 = list2[j][1] - list2[j][0]\n",
    "        numerator = min(diff1, diff2)\n",
    "        denominator = max(diff1, diff2)\n",
    "        overlap.append(float(numerator/denominator))\n",
    "\n",
    "      i += 1\n",
    "      j += 1\n",
    "\n",
    "    # if the current label in list2 appears before the current label in list1\n",
    "    # append list2's label to tags2\n",
    "    # and append the '0, 0' label to tags1\n",
    "    else:\n",
    "      tags1.append(\"0, 0\")\n",
    "      tags2.append(str(list2[j]))\n",
    "      j += 1\n",
    "\n",
    "  # find cohen's kappa using our two tags arrays\n",
    "  # if the tags arrays only contain 1 element (i.e. only has one label)\n",
    "  # cohen's kappa returns a NaN\n",
    "  # so the condition checks and corrects for that\n",
    "  textual_kappa = cohen_kappa_score(tags1, tags2)\n",
    "  if tags1 == tags2 and len(tags1) == 1:\n",
    "    textual_kappa = 1\n",
    "\n",
    "  # find the mean of the overlap agreement in the text\n",
    "  # or append NaN if no overlap\n",
    "  if overlap != []:\n",
    "    overlap_agreement = mean(overlap)\n",
    "  else:\n",
    "    overlap_agreement = np.nan\n",
    "\n",
    "  return [textual_kappa, overlap_agreement]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHKd5pV-99uy"
   },
   "source": [
    "Note: dropping NaN rows means that this code only checks for inter-annotator agreement if BOTH annotators have annotated the text. It does not account for cases where one annotator found metaphors in the text and another does not annotate anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6-6gmBFQEdv8"
   },
   "outputs": [],
   "source": [
    "compare_df = pd.merge(annotator1_df, annotator2_df, on=['text_id', 'text'], suffixes=('_1', '_2'))\n",
    "# annotator columns are redundant now\n",
    "compare_df = compare_df.drop(columns=['annotator_1', 'annotator_2'])\n",
    "# drop any rows without annotations\n",
    "compare_df = compare_df.dropna()\n",
    "\n",
    "# compare_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2_QYLdU42HZ",
    "outputId": "7bb48865-9146-48dc-9c97-78901b9da5d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check for how many rows are getting compared\n",
    "len(compare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tGTZRE5acY_U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "textual_agreement = []\n",
    "overlap_agreement = []\n",
    "\n",
    "for index, row in compare_df.iterrows():\n",
    "  agreement = compare_labels(row, 'label_1', 'label_2')\n",
    "  textual_agreement.append(agreement[0])\n",
    "  overlap_agreement.append(agreement[1])\n",
    "\n",
    "compare_df['textual_agreement'] = textual_agreement\n",
    "compare_df['overlap_agreement'] = overlap_agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRBcHVtjZWXr"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As per [this](https://datatab.net/tutorial/cohens-kappa) explanation of cohen's kappa, below is a guideline for how the result can be interpreted:\n",
    "* \\> 0.8 - almost perfect\n",
    "* \\> 0.6 - substantial\n",
    "* \\> 0.4 - moderate\n",
    "* \\> 0.2 - fair\n",
    "* 0 - 0.2 - slight\n",
    "* < 0 - poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRXKEtYnTwii",
    "outputId": "22810a1a-bcf3-4539-cb93-830c85860059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average textual agreement:  0.7757065536440007\n",
      "Average overlap agreement:  0.6375\n"
     ]
    }
   ],
   "source": [
    "print(\"Average textual agreement: \", compare_df['textual_agreement'].mean())\n",
    "print(\"Average overlap agreement: \", compare_df['overlap_agreement'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAIRWISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6-6gmBFQEdv8"
   },
   "outputs": [],
   "source": [
    "compare_df_1vs2 = pd.merge(annotator1_df, annotator2_df, on=['text_id', 'text'], suffixes=('_1', '_2'))\n",
    "# annotator columns are redundant now\n",
    "compare_df_1vs2 = compare_df_1vs2.drop(columns=['annotator_1', 'annotator_2'])\n",
    "# drop any rows without annotations\n",
    "compare_df_1vs2 = compare_df_1vs2.dropna()\n",
    "\n",
    "# compare_df_1vs2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2_QYLdU42HZ",
    "outputId": "7bb48865-9146-48dc-9c97-78901b9da5d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check for how many rows are getting compared\n",
    "len(compare_df_1vs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tGTZRE5acY_U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "textual_agreement_1vs2 = []\n",
    "overlap_agreement_1vs2 = []\n",
    "\n",
    "for index, row in compare_df_1vs2.iterrows():\n",
    "  agreement_1vs2 = compare_labels(row, 'label_1', 'label_2')\n",
    "  textual_agreement_1vs2.append(agreement_1vs2[0])\n",
    "  overlap_agreement_1vs2.append(agreement_1vs2[1])\n",
    "\n",
    "compare_df_1vs2['textual_agreement'] = textual_agreement_1vs2\n",
    "compare_df_1vs2['overlap_agreement'] = overlap_agreement_1vs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRXKEtYnTwii",
    "outputId": "22810a1a-bcf3-4539-cb93-830c85860059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average textual agreement between annotators 1 and 2:  0.7757065536440007\n",
      "Average overlap agreement between annotations 1 and 2:  0.6375\n"
     ]
    }
   ],
   "source": [
    "print(\"Average textual agreement between annotators 1 and 2: \", compare_df_1vs2['textual_agreement'].mean())\n",
    "print(\"Average overlap agreement between annotations 1 and 2: \", compare_df_1vs2['overlap_agreement'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 vs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6-6gmBFQEdv8"
   },
   "outputs": [],
   "source": [
    "compare_df_1vs3 = pd.merge(annotator1_df, annotator3_df, on=['text_id', 'text'], suffixes=('_1', '_3'))\n",
    "# annotator columns are redundant now\n",
    "compare_df_1vs3 = compare_df_1vs3.drop(columns=['annotator_1', 'annotator_3'])\n",
    "# drop any rows without annotations\n",
    "compare_df_1vs3 = compare_df_1vs3.dropna()\n",
    "\n",
    "# compare_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2_QYLdU42HZ",
    "outputId": "7bb48865-9146-48dc-9c97-78901b9da5d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check for how many rows are getting compared\n",
    "len(compare_df_1vs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tGTZRE5acY_U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "textual_agreement_1vs3 = []\n",
    "overlap_agreement_1vs3 = []\n",
    "\n",
    "for index, row in compare_df_1vs3.iterrows():\n",
    "  agreement_1vs3 = compare_labels(row, 'label_1', 'label_3')\n",
    "  textual_agreement_1vs3.append(agreement_1vs3[0])\n",
    "  overlap_agreement_1vs3.append(agreement_1vs3[1])\n",
    "\n",
    "compare_df_1vs3['textual_agreement'] = textual_agreement_1vs3\n",
    "compare_df_1vs3['overlap_agreement'] = overlap_agreement_1vs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRXKEtYnTwii",
    "outputId": "22810a1a-bcf3-4539-cb93-830c85860059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average textual agreement between annotators 1 and 3: 0.866639566587864\n",
      "Average overlap agreement between annotators 1 and 3: 0.4857142857142857\n"
     ]
    }
   ],
   "source": [
    "print(\"Average textual agreement between annotators 1 and 3:\", compare_df_1vs3['textual_agreement'].mean())\n",
    "print(\"Average overlap agreement between annotators 1 and 3:\", compare_df_1vs3['overlap_agreement'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 vs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "6-6gmBFQEdv8"
   },
   "outputs": [],
   "source": [
    "compare_df_2vs3 = pd.merge(annotator2_df, annotator3_df, on=['text_id', 'text'], suffixes=('_2', '_3'))\n",
    "# annotator columns are redundant now\n",
    "compare_df_2vs3 = compare_df_2vs3.drop(columns=['annotator_2', 'annotator_3'])\n",
    "# drop any rows without annotations\n",
    "compare_df_2vs3 = compare_df_2vs3.dropna()\n",
    "\n",
    "# compare_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2_QYLdU42HZ",
    "outputId": "7bb48865-9146-48dc-9c97-78901b9da5d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check for how many rows are getting compared\n",
    "len(compare_df_2vs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tGTZRE5acY_U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "C:\\Users\\romha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "textual_agreement_2vs3 = []\n",
    "overlap_agreement_2vs3 = []\n",
    "\n",
    "for index, row in compare_df_2vs3.iterrows():\n",
    "  agreement_2vs3 = compare_labels(row, 'label_2', 'label_3')\n",
    "  textual_agreement_2vs3.append(agreement_2vs3[0])\n",
    "  overlap_agreement_2vs3.append(agreement_2vs3[1])\n",
    "\n",
    "compare_df_2vs3['textual_agreement'] = textual_agreement_2vs3\n",
    "compare_df_2vs3['overlap_agreement'] = overlap_agreement_2vs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRXKEtYnTwii",
    "outputId": "22810a1a-bcf3-4539-cb93-830c85860059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average textual agreement between annotators 2 and 3: 0.8116449387955535\n",
      "Average overlap agreement: annotators 2 and 3: 0.875\n"
     ]
    }
   ],
   "source": [
    "print(\"Average textual agreement between annotators 2 and 3:\", compare_df_2vs3['textual_agreement'].mean())\n",
    "print(\"Average overlap agreement: annotators 2 and 3:\", compare_df_2vs3['overlap_agreement'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average textual agreement between annotators R and A: 0.7757065536440007\n",
      "Average overlap agreement between annotators R and A: 0.6375 \n",
      "\n",
      "Average textual agreement between annotators R and V: 0.866639566587864\n",
      "Average overlap agreement between annotators R and V: 0.4857142857142857 \n",
      "\n",
      "Average textual agreement between annotators A and V: 0.8116449387955535\n",
      "Average overlap agreement between annotators A and V: 0.875\n"
     ]
    }
   ],
   "source": [
    "print(\"Average textual agreement between annotators R and A:\", compare_df_1vs2['textual_agreement'].mean())\n",
    "print(\"Average overlap agreement between annotators R and A:\", compare_df_1vs2['overlap_agreement'].mean(), \"\\n\")\n",
    "print(\"Average textual agreement between annotators R and V:\", compare_df_1vs3['textual_agreement'].mean())\n",
    "print(\"Average overlap agreement between annotators R and V:\", compare_df_1vs3['overlap_agreement'].mean(), \"\\n\")\n",
    "print(\"Average textual agreement between annotators A and V:\", compare_df_2vs3['textual_agreement'].mean())\n",
    "print(\"Average overlap agreement between annotators A and V:\", compare_df_2vs3['overlap_agreement'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8179970196758061"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([compare_df_1vs2['textual_agreement'].mean(),\n",
    "         compare_df_1vs3['textual_agreement'].mean(), \n",
    "         compare_df_2vs3['textual_agreement'].mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6660714285714285"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([compare_df_1vs2['overlap_agreement'].mean(),\n",
    "         compare_df_1vs3['overlap_agreement'].mean(), \n",
    "         compare_df_2vs3['overlap_agreement'].mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
